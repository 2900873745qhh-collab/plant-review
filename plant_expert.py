import requests
import re

# ==========================================
# âš™ï¸ é…ç½®åŒºåŸŸ
# ==========================================

# é­”æ³•é…ç½® (ä½ çš„ç«¯å£ 15732)
PROXIES = {
    "http": "http://127.0.0.1:15732",
    "https": "http://127.0.0.1:15732"
}

# ä¼ªè£…å¤´éƒ¨ (éå¸¸é‡è¦ï¼æ²¡æœ‰è¿™ä¸ªç»´åŸºç™¾ç§‘ä¼šæ‹’ç»è¿æ¥)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) PlantReviewApp/1.0 (mailto:youremail@example.com)"
}

# ğŸ“– äººå·¥å­—å…¸ (ä½ çš„ä¿å‘½ç¬¦)
CUSTOM_DICTIONARY = {
    "é’è‹¹æœç«¹èŠ‹": "Goeppertia orbifolia",
    "å‘è´¢æ ‘": "Pachira glabra",
    "çº¢æ«": "Acer palmatum",
    "åœ†å¶åˆºè½´æ¦ˆ": "Licuala grandis",  # å¸®ä½ è¡¥ä¸Šäº†
    "éæ´²å‡Œéœ„": "Podranea ricasoliana",  # å¸®ä½ è¡¥ä¸Šäº†
    "æ©™é’ŸèŠ±": "Tecoma alata",  # å¸®ä½ è¡¥ä¸Šäº†
    "é»„é’ŸèŠ±": "Tecoma stans",
    "ç¡¬éª¨å‡Œéœ„": "Tecoma capensis",
    "è“æ˜ŸèŠ±": "Oxypetalum coeruleum"
}


# ==========================================
# æœç´¢å¼•æ“æ¨¡å—
# ==========================================

def search_bing_image(keyword):
    """
    ã€æ ¸æ­¦å™¨ã€‘Bing æœå›¾ (åªæœå­¦åï¼Œä¿è¯å‡†ç¡®ä¸”æœ‰å›¾)
    """
    try:
        url = "https://www.bing.com/images/search"
        # æœç´¢å­¦å + "plant" ç¡®ä¿ä¸‡æ— ä¸€å¤±
        params = {"q": f"{keyword} plant", "first": 1, "count": 1}
        # Bing ä¸éœ€è¦ä»£ç†é€šå¸¸ä¹Ÿèƒ½è¿ï¼Œå¦‚æœè¿ä¸ä¸Šä¼šè‡ªåŠ¨è·³è¿‡
        resp = requests.get(url, params=params, headers=HEADERS, timeout=5)

        # ä½¿ç”¨æ­£åˆ™æå–å›¾ç‰‡é“¾æ¥ (Bing çš„é¡µé¢ç»“æ„)
        # è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æå–é€»è¾‘ï¼Œé€šå¸¸èƒ½æ‹¿åˆ°ç¬¬ä¸€å¼ å¤§å›¾
        links = re.findall(r'murl&quot;:&quot;(.*?)&quot;', resp.text)
        if links:
            return links[0]
    except:
        pass
    return None


def get_latin_from_wikidata(chinese_name):
    """ã€ç¿»è¯‘å®˜ 1ã€‘Wikidata"""
    url = "https://www.wikidata.org/w/api.php"
    params = {
        "action": "wbsearchentities", "search": chinese_name,
        "language": "zh", "format": "json", "limit": 1
    }
    try:
        resp = requests.get(url, params=params, headers=HEADERS, timeout=5, proxies=PROXIES)
        data = resp.json()
        if not data.get("search"): return None
        entity_id = data["search"][0]["id"]

        ent_params = {"action": "wbgetentities", "ids": entity_id, "props": "claims", "format": "json"}
        ent_resp = requests.get(url, params=ent_params, headers=HEADERS, timeout=5, proxies=PROXIES)
        claims = ent_resp.json().get("entities", {}).get(entity_id, {}).get("claims", {})
        if "P225" in claims:
            return claims["P225"][0]["mainsnak"]["datavalue"]["value"]
    except:
        pass
    return None


def get_latin_from_inaturalist(chinese_name):
    """ã€ç¿»è¯‘å®˜ 2ã€‘iNaturalist"""
    url = "https://api.inaturalist.org/v1/taxa"
    params = {"q": chinese_name, "per_page": 3, "locale": "zh-CN", "taxon_id": 47126}
    try:
        resp = requests.get(url, params=params, headers=HEADERS, timeout=5)  # iNat ä¸€èˆ¬ä¸ç”¨ä»£ç†
        data = resp.json()
        if data['results']:
            for res in data['results']:
                if res['rank'] in ['species', 'variety', 'subspecies', 'hybrid']:
                    return res['name'], res.get('default_photo', {}).get('medium_url')
    except:
        pass
    return None, None


def _query_gbif(query_name):
    """ã€å›¾åº“ Aã€‘GBIF"""
    try:
        r1 = requests.get("https://api.gbif.org/v1/species/search",
                          params={"q": query_name, "limit": 1}, headers=HEADERS, timeout=5)
        d1 = r1.json()
        if not d1['results']: return None
        sp = d1['results'][0]
        # æ’é™¤éç‰©ç§
        if sp.get('rank') in ['CLASS', 'ORDER', 'PHYLUM', 'KINGDOM']: return None

        key = sp.get('key')
        result = {
            "name_cn": query_name,
            "scientific_name": sp.get('scientificName', 'æœªçŸ¥'),
            "family": sp.get('family', 'æœªçŸ¥'),
            "genus": sp.get('genus', 'æœªçŸ¥'),
            "image_url": None
        }

        # æœå›¾
        r2 = requests.get("https://api.gbif.org/v1/occurrence/search",
                          params={"taxonKey": key, "mediaType": "StillImage", "limit": 1},
                          headers=HEADERS, timeout=5)
        d2 = r2.json()
        if d2['results']:
            media = d2['results'][0].get('media', [])
            if media: result["image_url"] = media[0].get('identifier')
        return result
    except:
        return None


def get_image_from_wikimedia(scientific_name):
    """ã€å›¾åº“ Bã€‘Wiki"""
    url = "https://commons.wikimedia.org/w/api.php"
    params = {
        "action": "query", "generator": "search",
        "gsrsearch": f"{scientific_name} filetype:bitmap",
        "gsrlimit": 1, "prop": "imageinfo", "iiprop": "url", "format": "json"
    }
    try:
        # Wiki å¿…é¡»åŠ  User-Agent å¤´ï¼Œå¦åˆ™å¿…æŒ‚
        r = requests.get(url, params=params, headers=HEADERS, timeout=5, proxies=PROXIES)
        pages = r.json().get("query", {}).get("pages", {})
        for _, val in pages.items():
            return val.get("imageinfo", [{}])[0].get("url")
    except:
        pass
    return None


# ==========================================
# æ€»æŒ‡æŒ¥
# ==========================================

def fetch_plant_info(plant_name):
    latin_name = None
    fallback_image = None  # iNat å›¾

    print(f"    ğŸ” è§£æ [{plant_name}] ...", end="")

    # 1. æŸ¥å­—å…¸
    if plant_name in CUSTOM_DICTIONARY:
        latin_name = CUSTOM_DICTIONARY[plant_name]
        print(f" (å­—å…¸å‘½ä¸­: {latin_name})", end="")

    # 2. æŸ¥ Wiki
    if not latin_name:
        latin_name = get_latin_from_wikidata(plant_name)
        if latin_name: print(f" (Wikié”å®š: {latin_name})", end="")

    # 3. æŸ¥ iNat
    if not latin_name:
        latin_name, fallback_image = get_latin_from_inaturalist(plant_name)
        if latin_name: print(f" (iNaté”å®š: {latin_name})", end="")

    # å†³å®šç”¨ä»€ä¹ˆåå­—å»æœå›¾
    search_term = latin_name if latin_name else plant_name

    # --- æ„é€ åŸºç¡€ä¿¡æ¯ ---
    final_info = {
        "name_cn": plant_name,
        "scientific_name": search_term,
        "family": "æš‚æœªè·å–",
        "genus": search_term.split()[0] if search_term else "æœªçŸ¥",
        "image_url": None
    }

    # --- æœå›¾å¤§ä½œæˆ˜ ---

    # å°è¯• A: GBIF
    gbif_data = _query_gbif(search_term)
    if gbif_data and gbif_data['image_url']:
        final_info.update(gbif_data)
        final_info['name_cn'] = plant_name
        print(" -> GBIFæœ‰å›¾ âœ…")
        return final_info

    # å°è¯• B: Wiki (å¿…é¡»æœ‰å­¦å)
    if not final_info['image_url'] and latin_name:
        wiki_img = get_image_from_wikimedia(latin_name)
        if wiki_img:
            final_info['image_url'] = wiki_img
            print(" -> Wikiæœ‰å›¾ âœ…")
            return final_info

    # å°è¯• C: Bing (æœ€åçš„æ•‘æ˜Ÿ)
    if not final_info['image_url']:
        print(" -> ä¸“ä¸šåº“æ— å›¾ï¼Œå¯åŠ¨Bing...", end="")
        # å¦‚æœæœ‰å­¦åï¼Œç”¨å­¦åæœï¼›æ²¡æœ‰å­¦åï¼Œç”¨ä¸­æ–‡+æ¤ç‰©æœ
        bing_query = latin_name if latin_name else f"{plant_name} æ¤ç‰©"
        bing_img = search_bing_image(bing_query)
        if bing_img:
            final_info['image_url'] = bing_img
            final_info['family'] = "æ¥æº: Bingæœç´¢"
            print(" Bingæœ‰å›¾ âœ…")
            return final_info

    # å°è¯• D: iNat ä¿åº•
    if not final_info['image_url'] and fallback_image:
        final_info['image_url'] = fallback_image
        print(" -> iNatä¿åº• âœ…")
        return final_info

    print(" âŒ å½»åº•æ— å›¾")
    return None