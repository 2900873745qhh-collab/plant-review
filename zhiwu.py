
import os
from icrawler.builtin import BingImageCrawler


def download_from_file(txt_filename, output_subfolder):
    """
    è¯»å– txt æ–‡ä»¶ï¼Œä¸‹è½½å›¾ç‰‡åˆ° images ä¸‹çš„æŒ‡å®šå­æ–‡ä»¶å¤¹
    """
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(txt_filename):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {txt_filename}ï¼Œè¯·ç¡®è®¤å®ƒåœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ã€‚")
        return

    print(f"ğŸ“‚ å¼€å§‹å¤„ç†æ–‡ä»¶ï¼š{txt_filename} ...")

    # è¯»å–æ–‡ä»¶å†…å®¹
    with open(txt_filename, 'r', encoding='utf-8') as f:
        # è¯»å–æ¯ä¸€è¡Œï¼Œå»æ‰é¦–å°¾ç©ºæ ¼
        plant_names = [line.strip() for line in f.readlines() if line.strip()]

    total = len(plant_names)
    print(f"ğŸ“ å…±å‘ç° {total} ä¸ªæ¤ç‰©åç§°ï¼Œå‡†å¤‡å¼€å§‹ä¸‹è½½...")

    # éå†æ¯ä¸€ä¸ªæ¤ç‰©åå­—
    for index, name in enumerate(plant_names):
        print(f"[{index + 1}/{total}] æ­£åœ¨ä¸‹è½½ï¼š{name}")

        # è®¾å®šä¿å­˜è·¯å¾„ï¼šimages/å­æ–‡ä»¶å¤¹/æ¤ç‰©å
        # ä¾‹å¦‚ï¼šimages/important/é“¶æ
        save_path = os.path.join('images', output_subfolder, name)

        # å¦‚æœæ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œä¼šè‡ªåŠ¨åˆ›å»ºï¼ˆicrawler ä¼šå¤„ç†ï¼Œä½†ä¸ºäº†ä¿é™©æˆ‘ä»¬ä¹Ÿå¯ä»¥ä¸ç®¡ï¼‰

        # ä½¿ç”¨ Bing æœç´¢å¼•æ“ä¸‹è½½ï¼ˆæ¯” Google ç¨³å®šä¸éœ€è¦æ¢¯å­ï¼‰
        crawler = BingImageCrawler(storage={'root_dir': save_path})

        # å¼€å§‹ä¸‹è½½ï¼Œkeywordæ˜¯å…³é”®è¯ï¼Œmax_numæ˜¯ä¸‹è½½æ•°é‡ï¼ˆè¿™é‡Œè®¾ä¸º3å¼ ï¼‰
        # overwrite=True è¡¨ç¤ºå¦‚æœä¸å°å¿ƒé‡å¤ä¸‹è½½ä¼šè¦†ç›–ï¼Œé˜²æ­¢å ç”¨ç©ºé—´
        # ä¿®æ”¹åçš„ä»£ç  (åŠ ä¸Š " æ¤ç‰©" åç¼€)ï¼š
        crawler.crawl(keyword=name + " æ¤ç‰© èŠ±", max_num=3, overwrite=True)

    print(f"âœ… {txt_filename} å¤„ç†å®Œæˆï¼\n")


# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == '__main__':
    # 1. ä¸‹è½½ plants.txt åˆ° images/common æ–‡ä»¶å¤¹
    download_from_file('plants.txt', 'common')

    # 2. ä¸‹è½½ é‡ç‚¹.txt åˆ° images/important æ–‡ä»¶å¤¹
    download_from_file('é‡ç‚¹.txt', 'important')

    print("ğŸ‰ æ‰€æœ‰å›¾ç‰‡ä¸‹è½½ä»»åŠ¡ç»“æŸï¼è¯·å» images æ–‡ä»¶å¤¹æŸ¥çœ‹ç»“æœã€‚")